{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2382ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a16cb",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a03d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"Plots decision boundaries of model predicting on X in comparison to y.\n",
    "\n",
    "    Source - https://madewithml.com/courses/foundations/neural-networks/ (with modifications by Daniel Bourke from his course Pythorch for deep learning)\n",
    "    \"\"\"\n",
    "    # Put everything to CPU (works better with NumPy + Matplotlib)\n",
    "    model.to(\"cpu\")\n",
    "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "    # Setup prediction boundaries and grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "\n",
    "    # Make features\n",
    "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_logits = model(X_to_pred_on)\n",
    "\n",
    "    # Test for multi-class or binary and adjust logits to prediction labels\n",
    "    if len(torch.unique(y)) > 2:\n",
    "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # mutli-class\n",
    "    else:\n",
    "        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n",
    "\n",
    "    # Reshape preds and plot\n",
    "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b78a2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_metrics(y_true, y_pred, average=None):\n",
    "    \"\"\"\n",
    "    Create a simple plot with all four classification metrics.\n",
    "    Parameters:\n",
    "        y_true: the true value from the dataset\n",
    "        y_pred: the precitions from the model\n",
    "        avarage: used in case it is a multi-class classification\n",
    "    \"\"\"\n",
    "    metrics = [0,0,0,0]\n",
    "    metrics[0] = accuracy_score(y_true, y_pred)\n",
    "    metrics[1] = precision_score(y_true, y_pred, average)\n",
    "    metrics[2] = recall_score(y_true, y_pred, average)\n",
    "    metrics[3] = f1_score(y_true, y_pred, average)\n",
    "    \n",
    "    plt.bar(np.arange(4)-.1,metrics,.5)\n",
    "    plt.xticks([0,1,2,3],['Accuracy','Precision','Recall','F1-score'])\n",
    "    plt.ylim([.6,1])\n",
    "    plt.legend(['Metrics'])\n",
    "    plt.title('Performance metrics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c2d6f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_performance_metrics(y_train_true, y_test_true, y_train_pred, y_test_pred, average=\"weighted\"):\n",
    "    \"\"\"\n",
    "    Plot all classification metrics (accuracy, precision, recall and f1 score) in a bar chart.\n",
    "    Parameters:\n",
    "        y_train_true: true values from the training set\n",
    "        y_test_true: true values from the test set\n",
    "        y_train_pred: prediction from the model using the train set\n",
    "        y_test_pred: prediction from the model using the test set\n",
    "        \n",
    "        if you are using dataloaders, you can access the labels using train/test_loader.dataset.tensors[1] \n",
    "            to y_train_true/y_test_true parameters\n",
    "    \"\"\"\n",
    "    # initialize vectors\n",
    "    train_metrics = [0,0,0,0]\n",
    "    test_metrics  = [0,0,0,0]\n",
    "    \n",
    "    # training\n",
    "    train_metrics[0] = classification_accuracy(y_train_true, y_train_pred)\n",
    "    train_metrics[1] = classification_precision(y_train_true, y_train_pred, average=average)\n",
    "    train_metrics[2] = classification_recall(y_train_true, y_train_pred, average=average)\n",
    "    train_metrics[3] = classification_f1_score(y_train_true, y_train_pred, average=average)\n",
    "    \n",
    "    # test\n",
    "    test_metrics[0] = classification_accuracy(y_test_true, y_test_pred)\n",
    "    test_metrics[1] = classification_precision(y_test_true, y_test_pred, average=average)\n",
    "    test_metrics[2] = classification_recall(y_test_true, y_test_pred, average=average)\n",
    "    test_metrics[3] = classification_f1_score(y_test_true, y_test_pred, average=average)\n",
    "    \n",
    "    \n",
    "    plt.bar(np.arange(4)-.1,train_metrics,.5)\n",
    "    plt.bar(np.arange(4)+.1,test_metrics,.5)\n",
    "    plt.xticks([0,1,2,3],['Accuracy','Precision','Recall','F1-score'])\n",
    "    plt.ylim([.6,1])\n",
    "    plt.legend(['Train','Test'])\n",
    "    plt.title('Performance metrics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb9c86",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e442ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_names(metric_list: list):\n",
    "    \"\"\"\n",
    "    The function receives a list of substrings and it will return all metrics that contains the substrings.\n",
    "    Parameters:\n",
    "        metric_list (list): list with substrings of potential metric name.\n",
    "    return:\n",
    "        a list with all metrics that matches the substrings provided\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import SCORERS\n",
    "    \n",
    "    result = set()\n",
    "    metrics = []\n",
    "    for metric_substring in metric_list:\n",
    "        metrics = [i for i in SCORERS if metric_substring in i]\n",
    "        \n",
    "        for m in metrics:\n",
    "            result.add(m)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0948fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance_metrics_aprf(y_true, y_pred, average=None):\n",
    "    \"\"\"\n",
    "    Combine the main classification metrics accuracy, precision, recall and f1 score.\n",
    "    Parameters:\n",
    "        y_true: the true value from the dataset\n",
    "        y_pred: the precitions from the model\n",
    "        avarage: used in case it is a multi-class classification\n",
    "    \"\"\"\n",
    "    print(f\"Accuracy: {classification_accuracy(y_true, y_pred)*100:.2f}%\")\n",
    "    print(f\"Precision: {classification_precision(y_true, y_pred, average)*100:.2f}%\")\n",
    "    print(f\"Recall: {classification_recall(y_true, y_pred, average)*100:.2f}%\")\n",
    "    print(f\"F1 Score: {classification_f1_score(y_true, y_pred, average)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "414805dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_abs_scaling(X_train, X_test):\n",
    "    from sklearn.preprocessing import MaxAbsScaler\n",
    "    \n",
    "    scaler = MaxAbsScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4141f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(X_train, X_test):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b1b521d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaling(X_train, X_test):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66a9e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true, y_preds, average=None):\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true labels vs. y_pred labels\n",
    "    on a classification.\n",
    "    Parameters:\n",
    "        y_true: the true value from the dataset\n",
    "        y_pred: the precitions from the model\n",
    "        avarage: used in case it is a multi-class classification\n",
    "    Return:\n",
    "        metric_dict (dict): it returns a dictionary for future use.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                   \"precision\": round(precision, 2),\n",
    "                   \"recall\": round(recall, 2),\n",
    "                   \"f1\": round(f1, 2)}\n",
    "    print(f\"Acc: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 score: {f1:.2f}\")\n",
    "    \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbcc1fc",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f475dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scaling_features(X, y, scaler, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Function that splits the features into train and test, and in addition, performs feature scaling.\n",
    "    Parameters:\n",
    "        X: features from the dataset\n",
    "        y: label from the dataset\n",
    "        scaler: the type of feature scaling method applied. It can be 'min_max' for MinMaxScaler(), 'max_abs' for MaxAbsScaler(),\n",
    "                and 'std' for StandardScaler().\n",
    "    Return:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_splitain_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    if (scaler == \"min_max\"):\n",
    "        scaler = MinMaxScaler()\n",
    "    elif (scaler == \"max_abs\"):\n",
    "        scaler = MaxAbsScaler()\n",
    "    elif (scaler == \"std\"):\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "982235a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_Hot_Encoder(data: pd.DataFrame, categorical_features: list):\n",
    "    \"\"\"\n",
    "    This function provides OneHotEncoder solution from a list of feature. Just need to provide the list and it will return the\n",
    "    DataFrame with the extra columns.\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): data frame with all features\n",
    "        categorical_features (list): list with all categorical columns that need OneHotEncoder\n",
    "    Return:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer #make_column_transformer\n",
    "    \n",
    "    one_hot_enc = OneHotEncoder()\n",
    "    transformer = ColumnTransformer(\"onehot\", one_hot_enc, categorical_features, reminder=\"passthrough\")\n",
    "    transformed_X = transformer.fit_transform(data)\n",
    "    \n",
    "    return transformed_X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b834861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
