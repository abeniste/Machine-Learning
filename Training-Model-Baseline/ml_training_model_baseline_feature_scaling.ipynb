{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fecba0ca",
   "metadata": {},
   "source": [
    "#### Model training with an algorithm that requires feature scaling\n",
    "\n",
    "Let's take a look the basic process to fit a model in machine learning. We are going to assume that the data is already clean, \n",
    "since the purpose here is not perform exploratory data analysis but to create a model using scikit-learn framework. In this example, we will take a look on a model that requires feature scaling. In addition, we will still use the 3-stes idea: split the data into train, evaluate and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33029545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alberto.beniste\\.conda\\envs\\alberto_beniste\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## MODELS ##\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "## METRICS ##\n",
    "# classificaton models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# regression models\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_error\n",
    "\n",
    "# in case you want silence warnings\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\") #\"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7edb55",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f714f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance_metrics_aprf(y_true, y_pred, average=\"binary\"):\n",
    "    \"\"\"\n",
    "    Combine the main classification metrics accuracy, precision, recall and f1 score.\n",
    "    Parameters:\n",
    "        y_true: the true value from the dataset\n",
    "        y_pred: the precitions from the model\n",
    "        avarage: used in case it is a multi-class classification\n",
    "    \"\"\"\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred)*100:.2f}%\")\n",
    "    print(f\"Precision: {precision_score(y_true, y_pred, average=average)*100:.2f}%\")\n",
    "    print(f\"Recall: {recall_score(y_true, y_pred, average=average)*100:.2f}%\")\n",
    "    print(f\"F1 Score: {f1_score(y_true, y_pred, average=average)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01fb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_metrics(y_true, y_pred, average=\"binary\"):\n",
    "    \"\"\"\n",
    "    Create a simple plot with all four classification metrics.\n",
    "    Parameters:\n",
    "        y_true: the true value from the dataset\n",
    "        y_pred: the precitions from the model\n",
    "        avarage: used in case it is a multi-class classification\n",
    "    \"\"\"\n",
    "    metrics = [0,0,0,0]\n",
    "    metrics[0] = accuracy_score(y_true, y_pred)\n",
    "    metrics[1] = precision_score(y_true, y_pred, average=average)\n",
    "    metrics[2] = recall_score(y_true, y_pred, average=average)\n",
    "    metrics[3] = f1_score(y_true, y_pred, average=average)\n",
    "    print(metrics)\n",
    "    plt.bar(x=[0,1,2,3], height=metrics)\n",
    "    plt.xticks([0,1,2,3],['Accuracy','Precision','Recall','F1-score'])\n",
    "    plt.ylim([.6,1])\n",
    "    plt.legend(['Metrics'])\n",
    "    plt.title('Performance metrics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9550037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_names(metric_list: list):\n",
    "    \"\"\"\n",
    "    The function receives a list of substrings and it will return all metrics that contains the substrings.\n",
    "    Parameters:\n",
    "        metric_list (list): list with substrings of potential metric name.\n",
    "    return:\n",
    "        a list with all metrics that matches the substrings provided\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import SCORERS\n",
    "    \n",
    "    result = set()\n",
    "    metrics = []\n",
    "    for metric_substring in metric_list:\n",
    "        metrics = [i for i in SCORERS if metric_substring in i]\n",
    "        \n",
    "        for m in metrics:\n",
    "            result.add(m)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f230994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scaling_features(X, y, scaler, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Function that splits the features into train and test, and in addition, performs feature scaling.\n",
    "    Parameters:\n",
    "        X: features from the dataset\n",
    "        y: label from the dataset\n",
    "        scaler: the type of feature scaling method applied. It can be 'min_max' for MinMaxScaler(), 'max_abs' for MaxAbsScaler(),\n",
    "                and 'std' for StandardScaler().\n",
    "    Return:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    if (scaler == \"min_max\"):\n",
    "        scaler = MinMaxScaler()\n",
    "    elif (scaler == \"max_abs\"):\n",
    "        scaler = MaxAbsScaler()\n",
    "    elif (scaler == \"std\"):\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    # compute the statistics only to the train set\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298d50e",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797f9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the the dataset California housing mareket, provided by scikit-learn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2d2f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "           37.88      , -122.23      ],\n",
       "        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "           37.86      , -122.22      ],\n",
       "        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "           37.85      , -122.24      ],\n",
       "        ...,\n",
       "        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "           39.43      , -121.22      ],\n",
       "        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "           39.43      , -121.32      ],\n",
       "        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "           39.37      , -121.24      ]]),\n",
       " 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n",
       " 'frame': None,\n",
       " 'target_names': ['MedHouseVal'],\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block group\\n        - HouseAge      median house age in block group\\n        - AveRooms      average number of rooms per household\\n        - AveBedrms     average number of bedrooms per household\\n        - Population    block group population\\n        - AveOccup      average number of household members\\n        - Latitude      block group latitude\\n        - Longitude     block group longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nAn household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surpinsingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048fee64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude   \n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88  \\\n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the result into a dataframe\n",
    "house_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"])\n",
    "house_df[\"target\"] = pd.DataFrame(housing[\"target\"])\n",
    "house_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4662ae2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MedInc        0\n",
       "HouseAge      0\n",
       "AveRooms      0\n",
       "AveBedrms     0\n",
       "Population    0\n",
       "AveOccup      0\n",
       "Latitude      0\n",
       "Longitude     0\n",
       "target        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f9bd9",
   "metadata": {},
   "source": [
    "#### Step 1: Separate the features and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f72ea4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20640, 8), (20640,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a classification model to predict heart disease.\n",
    "X = house_df.drop(columns=\"target\")\n",
    "y = house_df[\"target\"]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767f94e",
   "metadata": {},
   "source": [
    "#### Step 2: Split the data into train and test. In addition, perform feature scaling.\n",
    "\n",
    "Cross-validation is a generic term that indicates a methodology to split the data into train (to create the model) and test (to test the performance of the model). The basic and simplest way is to split into only two sets: train and test datasets. In that case sacrifice part of the data for model performance evaluation. A more advanced methods for splitting the data is to use the 3-sets: split into train, evaluation (used to fine tuning the hyperparameters) and test (the truly unseen data used to perform the final test).\n",
    "\n",
    "A variation of train/evaluation/test sets is to use cros_val_score() or cross_validate() functions. We split into train and set datasets, but the difference is that we use the those functions to train and compute peforance accross the whole training set. We can have a much better idea of the true performance of the model \n",
    "\n",
    "Note that cross_validate function returns more information and it allows to use compute more performance metrics. That's what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e197b9",
   "metadata": {},
   "source": [
    "Feature scaling is mandatory for some algorithms that peform gradient descent. It transforms the features inot a small range of values. It is important to note that, as the name implies, ONLY the features are scaled. The labels/target does not need to be scaled. The three most common method to perform feature scaling is:\n",
    "* maximum absolute scaling\n",
    "* normalization scaling (min max scaling)\n",
    "* standardization (also called z-score normalization)\n",
    "\n",
    "To prevent data leakage we need to fit on train data, onlly then peform the scaling on train and test dataset. The fit() method compute the statistics used by the correspondent method and the transform() function apply the statistics metrics to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0f7bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the function split_scaling_features, defined above, to perform train/test split AND feature scaling\n",
    "# X_test and y_test will be used only to peform the final performance\n",
    "X_train, X_test, y_train, y_test = split_scaling_features(X, y, scaler=\"min_max\", test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d931a205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14448, 8), (6192, 8), (14448,), (6192,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8b38f",
   "metadata": {},
   "source": [
    "Step 3: Choose the model based on the type of problem. In this case, this is a classificaiton problem, and we use random forest estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c205d",
   "metadata": {},
   "source": [
    "#### Step 3: Choose the model\n",
    "\n",
    "We are going to use Ridge algorithm for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45eacc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge add in a penalty which is the alpha parameter.\n",
    "model = Ridge(alpha=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fb535b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg_mean_absolute_error',\n",
       " 'neg_mean_absolute_percentage_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_root_mean_squared_error'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use 5 folds (cv=5). The scoring parameters is the errors you want to measure. You need to pass the correct name. \n",
    "# Go to https://scikit-learn.org/stable/modules/model_evaluation.html for a complete list of metrics you can use.\n",
    "# You can also use the funciton above get_metric_names() that returns the metrics from a substring of metrics\n",
    "\n",
    "# I want metrics with contains error and root in their names. For this classification metrics. Let's see what it returns:\n",
    "get_metric_names([\"mean_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38410681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time we will use cross_val_score function. For an example for cross_validate, \n",
    "#    please refere to ml_training_model_baseline notebook.\n",
    "\n",
    "#scores_cv = cross_validate(model, X_train, y_train, scoring=[\"neg_mean_absolute_error\",\"neg_root_mean_squared_error\"], cv=5)\n",
    "#OR \n",
    "scores_cv = cross_val_score(model, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b280e8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7649308 , -0.73764669, -0.69416695, -0.76768818, -0.77553752,\n",
       "       -0.70542886, -0.75649899, -0.71170538, -0.71871456, -0.73783851])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All scorer objects follow the convention that higher return value are better accross different metrics.\n",
    "# So, they apply negative values\n",
    "scores_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5783355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7370156439700112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since the hyperparameter can vary from 0 to infinity, we will use cross-validation\n",
    "# We need to experiment for many aplha values.\n",
    "np.abs(np.mean(scores_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b693742",
   "metadata": {},
   "source": [
    "#### Step 4: Experiment/fine tune the mode with different hyperparameters\n",
    "\n",
    "We need to experiment with a variety of alpha values and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba199574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is another version of Ridge called RidgeCV. It peforms cross-validation for many alpha values\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8913daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It accepts a list of alpha values, and the cv parameter for the number of folds for the cross-validation\n",
    "# We also set the scoring parameter to set the metris used to select the best aplha.\n",
    "model_cv = RidgeCV(alphas=(0.05, 0.1, 0.5, 1, 5, 10), cv=10, scoring=\"neg_root_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b8f01e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeCV(alphas=(0.05, 0.1, 0.5, 1, 5, 10), cv=10,\n",
       "        scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV(alphas=(0.05, 0.1, 0.5, 1, 5, 10), cv=10,\n",
       "        scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RidgeCV(alphas=(0.05, 0.1, 0.5, 1, 5, 10), cv=10,\n",
       "        scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae42e513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it shows the alpha that performs the best\n",
    "model_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ec4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know the best aplha, we can compute the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92c222f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46c454e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7230868941321775"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we got a better model using RidgCV\n",
    "mean_squared_error(y_test, y_pred, squared=False) # squared false means to compute root mean square error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d5f4e",
   "metadata": {},
   "source": [
    "It is important to note here that maybe the data is not linear. Since Ridge is a linear model, it may not fit well with this type of data.\n",
    "That's the reason we have to experiment with other non-linear algorithm like RandomForestRegressor.\n",
    "\n",
    "Let's do a quick test using the default hyperparameter for RandomForestRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14881010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_rfr = RandomForestRegressor()\n",
    "model_rfr.fit(X_train, y_train)\n",
    "y_pred_rtf = model_rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "047bb298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5221420976132407"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we got a much better result than RidgeCV\n",
    "mean_squared_error(y_test, y_pred_rtf, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106ee9e",
   "metadata": {},
   "source": [
    "#### Step 5: Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b0c9d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['california_housing_model.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(model_rfr, 'california_housing_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6ec19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
